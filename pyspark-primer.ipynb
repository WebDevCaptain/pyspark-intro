{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e926ad-10db-426f-8098-70458bb5e5a7",
   "metadata": {},
   "source": [
    "# PySpark - 5 min intro\n",
    "\n",
    "PySpark (Python API for Apache Spark) - offers a faster, more flexible alternative to the traditional MapReduce framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787ccd5-5717-4aad-884e-ec36ca9d2886",
   "metadata": {},
   "source": [
    "## RDD - Resilient Distributed Dataset\n",
    "\n",
    "- It is the fundamental data structure in Spark.\n",
    "- Think of it as a distributed collection of elements that can be processed in parallel.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**RDDs are fault-tolerant**, **immutable**, and can be operated on using two types of operations:\n",
    "\n",
    "- **Transformations**: Create a new RDD from an existing one (e.g., map(), filter(), flatMap()).\n",
    "\n",
    "- **Actions**: Return a value to the driver program or write data to an external storage system (e.g., collect(), count(), reduce())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf35c1f-b87d-45d9-bec3-b8ba7dca0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 18:09:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize a SparkContext\n",
    "# This is the entry point to Spark functionality.\n",
    "sc = SparkContext(\"local\", \"PySparkBasics-5min-Primer\")\n",
    "\n",
    "# Create an RDD from a list of numbers (our sample dataset)\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d85065-8407-41cd-8415-61cb89a352c2",
   "metadata": {},
   "source": [
    "### RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e507398-fc43-4b29-a3ff-d0b7d30c6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubled Numbers: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Multiply each element by 2\n",
    "doubled = numbers.map(lambda x: x * 2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Doubled Numbers:\", doubled.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95f4ac-b7b2-4fc6-ad8b-31a2e8906093",
   "metadata": {},
   "source": [
    "> `collect()` gathers the results from all nodes back to the driver program. Not recommended for big data (for our toy dataset, it's fine). Use `.take()` for inspecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f2ed6e-85af-4ba8-ac1b-7bb46dd18b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3d7578-8596-417e-a7b1-1866679c9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers > 5: [6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Filter out numbers greater than 5, returns a new RDD\n",
    "filtered = numbers.filter(lambda x: x > 5)\n",
    "\n",
    "# Print the filtered numbers\n",
    "print(\"Numbers > 5:\", filtered.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459ffbbb-a64d-4e0b-b576-3a91c1db7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Numbers: 55\n"
     ]
    }
   ],
   "source": [
    "# Sum all the numbers in the RDD\n",
    "sum_numbers = numbers.reduce(lambda a, b: a + b)\n",
    "\n",
    "# Print the sum\n",
    "print(\"Sum of Numbers:\", sum_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "661036a2-8242-4999-a00d-c27fd27979aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['PySpark', 'is', 'fun', 'MapReduce', 'is', 'old', 'school', 'Spark', 'offers', 'more', 'functionality']\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of sentences\n",
    "sentences = sc.parallelize([\n",
    "    \"PySpark is fun\",\n",
    "    \"MapReduce is old school\",\n",
    "    \"Spark offers more functionality\"\n",
    "])\n",
    "\n",
    "# Split each sentence into words and flatten the result\n",
    "words = sentences.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "# Print the words\n",
    "print(\"Words:\", words.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dea36-bbc7-4f93-b410-af34739a27c4",
   "metadata": {},
   "source": [
    "### Chaining transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eed7f1e-da44-4a25-84ba-ab7688cd3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Even Numbers: [4, 16, 36, 64, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 18:09:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Chain multiple operations: square even numbers from our list\n",
    "result = numbers.filter(lambda x: x % 2 == 0) \\\n",
    "                .map(lambda x: x * x)  # square the numbers\n",
    "\n",
    "# Execute the pipeline and collect the results\n",
    "print(\"Squared Even Numbers:\", result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd7d52-b044-463c-a301-f556872dedb7",
   "metadata": {},
   "source": [
    "> `.collect()` triggers the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d998e6a-c2a1-4be6-8b64-627af64ac859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3974b53-adfa-437e-81e0-3d1da9100771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
