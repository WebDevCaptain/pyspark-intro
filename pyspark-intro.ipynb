{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 16:46:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkIntro\").getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `inferSchema=True` allows Spark to guess the data type for each column automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+\n",
      "| id|   name|age|salary| department|\n",
      "+---+-------+---+------+-----------+\n",
      "|  1|  Alice| 30| 70000|         HR|\n",
      "|  2|    Bob| 35| 80000|Engineering|\n",
      "|  3|Charlie| 25| 50000|  Marketing|\n",
      "|  4|  David| 40| 90000|Engineering|\n",
      "|  5|    Eva| 28| 60000|         HR|\n",
      "|  6|  Frank| 32| 75000|  Marketing|\n",
      "|  7|   Gina| 27| 55000|Engineering|\n",
      "|  8|  Harry| 31| 70000|         HR|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|\n",
      "| 10|   Jack| 33| 80000|Engineering|\n",
      "| 11|   Kate| 26| 50000|         HR|\n",
      "| 12|   Lily| 34| 75000|  Marketing|\n",
      "| 13|   Mike| 28| 60000|Engineering|\n",
      "| 14|  Nancy| 30| 70000|         HR|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|\n",
      "+---+-------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data/sample.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the DataFrame schema to see the inferred column types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name| department|\n",
      "+-------+-----------+\n",
      "|  Alice|         HR|\n",
      "|    Bob|Engineering|\n",
      "|Charlie|  Marketing|\n",
      "|  David|Engineering|\n",
      "|    Eva|         HR|\n",
      "|  Frank|  Marketing|\n",
      "|   Gina|Engineering|\n",
      "|  Harry|         HR|\n",
      "|    Ivy|  Marketing|\n",
      "|   Jack|Engineering|\n",
      "|   Kate|         HR|\n",
      "|   Lily|  Marketing|\n",
      "|   Mike|Engineering|\n",
      "|  Nancy|         HR|\n",
      "|  Oscar|  Marketing|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the 'name' and 'department' columns from the DataFrame\n",
    "df.select(\"name\", \"department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-----------+\n",
      "| id| name|age|salary| department|\n",
      "+---+-----+---+------+-----------+\n",
      "|  1|Alice| 30| 70000|         HR|\n",
      "|  2|  Bob| 35| 80000|Engineering|\n",
      "|  4|David| 40| 90000|Engineering|\n",
      "|  6|Frank| 32| 75000|  Marketing|\n",
      "|  8|Harry| 31| 70000|         HR|\n",
      "| 10| Jack| 33| 80000|Engineering|\n",
      "| 12| Lily| 34| 75000|  Marketing|\n",
      "| 14|Nancy| 30| 70000|         HR|\n",
      "| 15|Oscar| 32| 80000|  Marketing|\n",
      "+---+-----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only employees with a salary greater than 65000\n",
    "df.filter(df[\"salary\"] > 65000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+\n",
      "| id|   name|age|salary| department|\n",
      "+---+-------+---+------+-----------+\n",
      "|  3|Charlie| 25| 50000|  Marketing|\n",
      "|  5|    Eva| 28| 60000|         HR|\n",
      "|  7|   Gina| 27| 55000|Engineering|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|\n",
      "| 11|   Kate| 26| 50000|         HR|\n",
      "| 13|   Mike| 28| 60000|Engineering|\n",
      "+---+-------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only employees with a salary less than 65000\n",
    "df.filter(df[\"salary\"] < 65000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+-------------+\n",
      "| id|   name|age|salary| department|double_salary|\n",
      "+---+-------+---+------+-----------+-------------+\n",
      "|  1|  Alice| 30| 70000|         HR|       140000|\n",
      "|  2|    Bob| 35| 80000|Engineering|       160000|\n",
      "|  3|Charlie| 25| 50000|  Marketing|       100000|\n",
      "|  4|  David| 40| 90000|Engineering|       180000|\n",
      "|  5|    Eva| 28| 60000|         HR|       120000|\n",
      "|  6|  Frank| 32| 75000|  Marketing|       150000|\n",
      "|  7|   Gina| 27| 55000|Engineering|       110000|\n",
      "|  8|  Harry| 31| 70000|         HR|       140000|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|       120000|\n",
      "| 10|   Jack| 33| 80000|Engineering|       160000|\n",
      "| 11|   Kate| 26| 50000|         HR|       100000|\n",
      "| 12|   Lily| 34| 75000|  Marketing|       150000|\n",
      "| 13|   Mike| 28| 60000|Engineering|       120000|\n",
      "| 14|  Nancy| 30| 70000|         HR|       140000|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|       160000|\n",
      "+---+-------+---+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column 'double_salary' that is twice the 'salary'\n",
    "df_with_new_column = df.withColumn(\"double_salary\", col(\"salary\") * 2)\n",
    "df_with_new_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| department|avg(salary)|\n",
      "+-----------+-----------+\n",
      "|Engineering|    73000.0|\n",
      "|         HR|    64000.0|\n",
      "|  Marketing|    68000.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'department' and compute the average salary\n",
    "df.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|employee_count|\n",
      "+-----------+--------------+\n",
      "|Engineering|             5|\n",
      "|         HR|             5|\n",
      "|  Marketing|             5|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view from the DataFrame\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Run an SQL query to count the number of employees in each department\n",
    "sql_query = \"\"\"\n",
    "SELECT department, COUNT(*) as employee_count\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "\"\"\"\n",
    "result = spark.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:46:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-adffdc3d-830f-4286-af4e-cd24f9768cd5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/02/01 16:46:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/02/01 16:46:42 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|    5|\n",
      "|         HR|    5|\n",
      "|  Marketing|    5|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|   10|\n",
      "|         HR|   10|\n",
      "|  Marketing|   10|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|   15|\n",
      "|         HR|   15|\n",
      "|  Marketing|   15|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ms/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ms/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ms/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m query \u001b[38;5;241m=\u001b[39m aggregated_stream\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Wait for the streaming query to finish (press Ctrl+C to stop)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Machine-Learning/learn/PySpark/pyspark-intro/env/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the schema based on our sample data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read streaming data from the 'streaming_data' directory\n",
    "stream_df = spark.readStream.option(\"header\", True).schema(schema).csv(\"streaming_data/\")  #  This dir must be present\n",
    "\n",
    "# Let's group by department and count employees\n",
    "aggregated_stream = stream_df.groupBy(\"department\").count()\n",
    "\n",
    "# Write the streaming output to the console\n",
    "query = aggregated_stream.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "# Wait for the streaming query to finish (press Ctrl+C to stop)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's predict salary using age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "|age|salary|features|\n",
      "+---+------+--------+\n",
      "| 30| 70000|  [30.0]|\n",
      "| 35| 80000|  [35.0]|\n",
      "| 25| 50000|  [25.0]|\n",
      "| 40| 90000|  [40.0]|\n",
      "| 28| 60000|  [28.0]|\n",
      "| 32| 75000|  [32.0]|\n",
      "| 27| 55000|  [27.0]|\n",
      "| 31| 70000|  [31.0]|\n",
      "| 29| 60000|  [29.0]|\n",
      "| 33| 80000|  [33.0]|\n",
      "| 26| 50000|  [26.0]|\n",
      "| 34| 75000|  [34.0]|\n",
      "| 28| 60000|  [28.0]|\n",
      "| 30| 70000|  [30.0]|\n",
      "| 32| 80000|  [32.0]|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:49:16 WARN Instrumentation: [bd4e989b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/02/01 16:49:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/01 16:49:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/02/01 16:49:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+\n",
      "|age|salary|        prediction|\n",
      "+---+------+------------------+\n",
      "| 25| 50000| 53208.23244552073|\n",
      "| 27| 55000|58740.920096852395|\n",
      "| 29| 60000| 64273.60774818408|\n",
      "| 30| 70000| 67039.95157384992|\n",
      "+---+------+------------------+\n",
      "\n",
      "Coefficients: [2766.3438256658396]\n",
      "Intercept: -15950.363196125269\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create a new DataFrame with the necessary columns ('age' as the feature, 'salary' as the label)\n",
    "assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"features\")\n",
    "ml_df = assembler.transform(df)\n",
    "\n",
    "# Display the new DataFrame with the 'features' column\n",
    "ml_df.select(\"age\", \"salary\", \"features\").show()\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"age\", \"salary\", \"prediction\").show()\n",
    "\n",
    "# Print the coefficients and intercept for the linear regression model\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
