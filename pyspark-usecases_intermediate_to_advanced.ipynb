{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97c4c4a-a93b-41ea-a7af-499cfeae8199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 19:45:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/01 19:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/01 19:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkSteppingUp\").getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3141f5c5-6b71-43db-8542-2c288e63c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+\n",
      "| id|   name|age|salary| department|\n",
      "+---+-------+---+------+-----------+\n",
      "|  1|  Alice| 30| 70000|         HR|\n",
      "|  2|    Bob| 35| 80000|Engineering|\n",
      "|  3|Charlie| 25| 50000|  Marketing|\n",
      "|  4|  David| 40| 90000|Engineering|\n",
      "|  5|    Eva| 28| 60000|         HR|\n",
      "|  6|  Frank| 32| 75000|  Marketing|\n",
      "|  7|   Gina| 27| 55000|Engineering|\n",
      "|  8|  Harry| 31| 70000|         HR|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|\n",
      "| 10|   Jack| 33| 80000|Engineering|\n",
      "| 11|   Kate| 26| 50000|         HR|\n",
      "| 12|   Lily| 34| 75000|  Marketing|\n",
      "| 13|   Mike| 28| 60000|Engineering|\n",
      "| 14|  Nancy| 30| 70000|         HR|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|\n",
      "+---+-------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data/sample.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34706f6e-a2db-4319-a549-046e347b95cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbc545-cf72-48ca-be2e-09f282adfcb2",
   "metadata": {},
   "source": [
    "## Window Functions - More advanced \"Aggregations\"\n",
    "\n",
    "> Window functions allow you to perform calculations across a set of rows that are related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e191b9f-3054-45e7-a9d3-83a71133f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+-----------+\n",
      "| id|   name|age|salary| department|salary_rank|\n",
      "+---+-------+---+------+-----------+-----------+\n",
      "|  4|  David| 40| 90000|Engineering|          1|\n",
      "|  2|    Bob| 35| 80000|Engineering|          2|\n",
      "| 10|   Jack| 33| 80000|Engineering|          2|\n",
      "| 13|   Mike| 28| 60000|Engineering|          4|\n",
      "|  7|   Gina| 27| 55000|Engineering|          5|\n",
      "|  1|  Alice| 30| 70000|         HR|          1|\n",
      "|  8|  Harry| 31| 70000|         HR|          1|\n",
      "| 14|  Nancy| 30| 70000|         HR|          1|\n",
      "|  5|    Eva| 28| 60000|         HR|          4|\n",
      "| 11|   Kate| 26| 50000|         HR|          5|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|          1|\n",
      "|  6|  Frank| 32| 75000|  Marketing|          2|\n",
      "| 12|   Lily| 34| 75000|  Marketing|          2|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|          4|\n",
      "|  3|Charlie| 25| 50000|  Marketing|          5|\n",
      "+---+-------+---+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Define a window partitioned by 'department' and ordered by 'salary' in descending order\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply the window function to compute the rank of employees within their department\n",
    "df_with_rank = df.withColumn(\"salary_rank\", rank().over(window_spec))\n",
    "df_with_rank.show()\n",
    "\n",
    "# The 'Window.partitionBy' divides data into groups (departments),\n",
    "# and 'orderBy' sorts employees within each group by salary in desc order. \n",
    "# The 'rank' function assigns a rank to each row based on the order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5f69b-f50b-4a0a-b405-c68c99ae6c27",
   "metadata": {},
   "source": [
    "## Custom transformations using User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5276f6bd-d89f-4dee-b0fd-7433eafd94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+---------------+\n",
      "| id|   name|age|salary| department|salary_category|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "|  1|  Alice| 30| 70000|         HR|         Medium|\n",
      "|  2|    Bob| 35| 80000|Engineering|           High|\n",
      "|  3|Charlie| 25| 50000|  Marketing|            Low|\n",
      "|  4|  David| 40| 90000|Engineering|           High|\n",
      "|  5|    Eva| 28| 60000|         HR|         Medium|\n",
      "|  6|  Frank| 32| 75000|  Marketing|         Medium|\n",
      "|  7|   Gina| 27| 55000|Engineering|            Low|\n",
      "|  8|  Harry| 31| 70000|         HR|         Medium|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|         Medium|\n",
      "| 10|   Jack| 33| 80000|Engineering|           High|\n",
      "| 11|   Kate| 26| 50000|         HR|            Low|\n",
      "| 12|   Lily| 34| 75000|  Marketing|         Medium|\n",
      "| 13|   Mike| 28| 60000|Engineering|         Medium|\n",
      "| 14|  Nancy| 30| 70000|         HR|         Medium|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|           High|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# custom function that classifies salary\n",
    "def classify_salary(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"High\"\n",
    "    elif salary >= 60000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "classify_salary_udf = udf(classify_salary, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column 'salary_category'\n",
    "df_with_category = df.withColumn(\"salary_category\", classify_salary_udf(col(\"salary\")))\n",
    "df_with_category.show()\n",
    "\n",
    "# The UDF 'classify_salary' categorizes each employee's salary.\n",
    "# UDFs should be used when built-in functions do not meet our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3ab53-d09e-41de-8eb2-2bc26c1ea457",
   "metadata": {},
   "source": [
    "### Efficient Join operations - using Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c625d6-a0ee-4b21-82da-e3a77b17c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------+---+------+-------------+\n",
      "| department| id|   name|age|salary|     location|\n",
      "+-----------+---+-------+---+------+-------------+\n",
      "|         HR|  1|  Alice| 30| 70000|     New York|\n",
      "|Engineering|  2|    Bob| 35| 80000|San Francisco|\n",
      "|  Marketing|  3|Charlie| 25| 50000|      Chicago|\n",
      "|Engineering|  4|  David| 40| 90000|San Francisco|\n",
      "|         HR|  5|    Eva| 28| 60000|     New York|\n",
      "|  Marketing|  6|  Frank| 32| 75000|      Chicago|\n",
      "|Engineering|  7|   Gina| 27| 55000|San Francisco|\n",
      "|         HR|  8|  Harry| 31| 70000|     New York|\n",
      "|  Marketing|  9|    Ivy| 29| 60000|      Chicago|\n",
      "|Engineering| 10|   Jack| 33| 80000|San Francisco|\n",
      "|         HR| 11|   Kate| 26| 50000|     New York|\n",
      "|  Marketing| 12|   Lily| 34| 75000|      Chicago|\n",
      "|Engineering| 13|   Mike| 28| 60000|San Francisco|\n",
      "|         HR| 14|  Nancy| 30| 70000|     New York|\n",
      "|  Marketing| 15|  Oscar| 32| 80000|      Chicago|\n",
      "+-----------+---+-------+---+------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a small DataFrame for department details\n",
    "dept_data = [\n",
    "    Row(department=\"HR\", location=\"New York\"),\n",
    "    Row(department=\"Engineering\", location=\"San Francisco\"),\n",
    "    Row(department=\"Marketing\", location=\"Chicago\")\n",
    "]\n",
    "dept_df = spark.createDataFrame(dept_data)\n",
    "\n",
    "# Perform a broadcast join between the employee DataFrame and the department DataFrame\n",
    "joined_df = df.join(broadcast(dept_df), on=\"department\", how=\"left\")\n",
    "joined_df.show()\n",
    "\n",
    "# The 'broadcast' function hints to Spark that dept_df is small, \n",
    "\n",
    "# and it should be broadcasted to all worker nodes for a more efficient join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f502ed4-b2df-4d3c-a0bd-6d727ffc1eac",
   "metadata": {},
   "source": [
    "### Caching and Persisting intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aaf5b41-ea50-4d9f-9349-4a2b424c84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+---------------+\n",
      "| id|   name|age|salary| department|salary_category|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "|  1|  Alice| 30| 70000|         HR|         Medium|\n",
      "|  2|    Bob| 35| 80000|Engineering|           High|\n",
      "|  3|Charlie| 25| 50000|  Marketing|            Low|\n",
      "|  4|  David| 40| 90000|Engineering|           High|\n",
      "|  5|    Eva| 28| 60000|         HR|         Medium|\n",
      "|  6|  Frank| 32| 75000|  Marketing|         Medium|\n",
      "|  7|   Gina| 27| 55000|Engineering|            Low|\n",
      "|  8|  Harry| 31| 70000|         HR|         Medium|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|         Medium|\n",
      "| 10|   Jack| 33| 80000|Engineering|           High|\n",
      "| 11|   Kate| 26| 50000|         HR|            Low|\n",
      "| 12|   Lily| 34| 75000|  Marketing|         Medium|\n",
      "| 13|   Mike| 28| 60000|Engineering|         Medium|\n",
      "| 14|  Nancy| 30| 70000|         HR|         Medium|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|           High|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame after performing a transformation\n",
    "cached_df = df_with_category.cache()\n",
    "cached_df.show()\n",
    "\n",
    "# Using 'cache()' will persist the DataFrame in memory, reducing the time needed for subsequent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df50c7d-358e-4d91-b44e-6f021b7b7103",
   "metadata": {},
   "source": [
    "## More analytics with DataFrames :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b21950-5e52-4576-962c-5b6071ea6a89",
   "metadata": {},
   "source": [
    "### Pivoting Data for aggregation\n",
    "\n",
    "Pivot operations allows us to transform data from rows to columns, which is useful for creating summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ff2bfd-415b-4307-ab4e-40e12a25128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+-----------------+\n",
      "| department|    Low| Medium|             High|\n",
      "+-----------+-------+-------+-----------------+\n",
      "|Engineering|55000.0|60000.0|83333.33333333333|\n",
      "|         HR|50000.0|67500.0|             NULL|\n",
      "|  Marketing|50000.0|70000.0|          80000.0|\n",
      "+-----------+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df_with_category.groupBy(\"department\") \\\n",
    "    .pivot(\"salary_category\", [\"Low\", \"Medium\", \"High\"]) \\\n",
    "    .avg(\"salary\")\n",
    "\n",
    "pivot_df.show()\n",
    "\n",
    "# 'pivot()' reshapes the DataFrame by pivoting on the 'salary_category'.\n",
    "# This creates columns for each category with the average salary as values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7600d6-d92a-4e20-8a48-d41912de407f",
   "metadata": {},
   "source": [
    "#### Filtering with multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36689331-f396-476f-ba26-ea3393c480de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-----------+---------------+\n",
      "| id| name|age|salary| department|salary_category|\n",
      "+---+-----+---+------+-----------+---------------+\n",
      "|  2|  Bob| 35| 80000|Engineering|           High|\n",
      "|  4|David| 40| 90000|Engineering|           High|\n",
      "|  7| Gina| 27| 55000|Engineering|            Low|\n",
      "| 10| Jack| 33| 80000|Engineering|           High|\n",
      "| 13| Mike| 28| 60000|Engineering|         Medium|\n",
      "| 15|Oscar| 32| 80000|  Marketing|           High|\n",
      "+---+-----+---+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# Filter employees who are either in 'Engineering' or have a 'High' salary category\n",
    "advanced_filter_df = df_with_category.filter(\n",
    "    (col(\"department\") == \"Engineering\") | (col(\"salary_category\") == \"High\")\n",
    ")\n",
    "advanced_filter_df.show()\n",
    "\n",
    "# The expression uses logical OR (|) to combine filtering conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df31765-2ff2-4571-add3-a1b7bb39b7c3",
   "metadata": {},
   "source": [
    "## ML pipelines with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e605b442-01cd-42ec-8706-3e267f181612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 19:45:24 WARN Instrumentation: [673185dc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/02/01 19:45:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/01 19:45:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/02/01 19:45:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+------+------------------+\n",
      "|   name| department|age|salary|        prediction|\n",
      "+-------+-----------+---+------+------------------+\n",
      "|  Alice|         HR| 30| 70000|  67026.9058295965|\n",
      "|    Bob|Engineering| 35| 80000| 80264.57399103155|\n",
      "|Charlie|  Marketing| 25| 50000|   51654.708520179|\n",
      "|  David|Engineering| 40| 90000| 95399.10313901394|\n",
      "|    Eva|         HR| 28| 60000|60973.094170403536|\n",
      "|  Frank|  Marketing| 32| 75000| 72843.04932735435|\n",
      "|   Gina|Engineering| 27| 55000|56049.327354259716|\n",
      "|  Harry|         HR| 31| 70000| 70053.81165919297|\n",
      "|    Ivy|  Marketing| 29| 60000| 63762.33183856492|\n",
      "|   Jack|Engineering| 33| 80000| 74210.76233183859|\n",
      "|   Kate|         HR| 26| 50000| 54919.28251121058|\n",
      "|   Lily|  Marketing| 34| 75000| 78896.86098654731|\n",
      "|   Mike|Engineering| 28| 60000|59076.233183856195|\n",
      "|  Nancy|         HR| 30| 70000|  67026.9058295965|\n",
      "|  Oscar|  Marketing| 32| 80000| 72843.04932735435|\n",
      "+-------+-----------+---+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Assuming our dataset is enhanced with categorical features (using 'department' here)\n",
    "# Step 1: Convert the 'department' column to numeric indices\n",
    "dept_indexer = StringIndexer(inputCol=\"department\", outputCol=\"dept_index\")\n",
    "\n",
    "# Step 2: One-hot encode the indexed department\n",
    "dept_encoder = OneHotEncoder(inputCol=\"dept_index\", outputCol=\"dept_vec\")\n",
    "\n",
    "# Step 3: Assemble features (using age and the one-hot encoded department)\n",
    "assembler = VectorAssembler(inputCols=[\"age\", \"dept_vec\"], outputCol=\"features\")\n",
    "\n",
    "# Step 4: Initialize the linear regression estimator (predicting salary)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "\n",
    "# Create the pipeline with all stages\n",
    "pipeline = Pipeline(stages=[dept_indexer, dept_encoder, assembler, lr])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "predictions = pipeline_model.transform(df)\n",
    "predictions.select(\"name\", \"department\", \"age\", \"salary\", \"prediction\").show()\n",
    "\n",
    "\n",
    "# This pipeline preprocesses categorical data and numerical features before applying linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001d5d5-d9b7-4614-af6a-af4fec364253",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5207853a-58d6-4098-a886-4fd9ba40ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 19:45:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+------+------------------+\n",
      "|   name| department|age|salary|        prediction|\n",
      "+-------+-----------+---+------+------------------+\n",
      "|  Alice|         HR| 30| 70000|  67026.7185513027|\n",
      "|    Bob|Engineering| 35| 80000|  80264.6054316039|\n",
      "|Charlie|  Marketing| 25| 50000| 51654.95601220027|\n",
      "|  David|Engineering| 40| 90000|  95398.9054438366|\n",
      "|    Eva|         HR| 28| 60000|60972.998546409624|\n",
      "|  Frank|  Marketing| 32| 75000| 72842.97602932606|\n",
      "|   Gina|Engineering| 27| 55000| 56049.72541203156|\n",
      "|  Harry|         HR| 31| 70000| 70053.57855374925|\n",
      "|    Ivy|  Marketing| 29| 60000| 63762.39602198644|\n",
      "|   Jack|Engineering| 33| 80000| 74210.88542671081|\n",
      "|   Kate|         HR| 26| 50000| 54919.27854151654|\n",
      "|   Lily|  Marketing| 34| 75000| 78896.69603421915|\n",
      "|   Mike|Engineering| 28| 60000|59076.585414478104|\n",
      "|  Nancy|         HR| 30| 70000|  67026.7185513027|\n",
      "|  Oscar|  Marketing| 32| 80000| 72842.97602932606|\n",
      "+-------+-----------+---+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a parameter grid for tuning the linear regression model\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define a regression evaluator (using RMSE as the metric)\n",
    "evaluator = RegressionEvaluator(labelCol=\"salary\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up the CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3  # Usingg 3-fold cross-validation\n",
    ")\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cv_model = crossval.fit(df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Make predictions using the best model found\n",
    "cv_predictions = best_model.transform(df)\n",
    "cv_predictions.select(\"name\", \"department\", \"age\", \"salary\", \"prediction\").show()\n",
    "\n",
    "\n",
    "# CrossValidator tests different parameter combinations defined in paramGrid.\n",
    "\n",
    "# The best model is selected based on the lowest RMSE (Root-Mean squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadbed8-303b-44dc-b638-eee981072da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
