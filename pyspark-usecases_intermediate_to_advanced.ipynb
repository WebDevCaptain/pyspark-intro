{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97c4c4a-a93b-41ea-a7af-499cfeae8199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 19:01:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/01 19:01:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/01 19:01:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkSteppingUp\").getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3141f5c5-6b71-43db-8542-2c288e63c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+\n",
      "| id|   name|age|salary| department|\n",
      "+---+-------+---+------+-----------+\n",
      "|  1|  Alice| 30| 70000|         HR|\n",
      "|  2|    Bob| 35| 80000|Engineering|\n",
      "|  3|Charlie| 25| 50000|  Marketing|\n",
      "|  4|  David| 40| 90000|Engineering|\n",
      "|  5|    Eva| 28| 60000|         HR|\n",
      "|  6|  Frank| 32| 75000|  Marketing|\n",
      "|  7|   Gina| 27| 55000|Engineering|\n",
      "|  8|  Harry| 31| 70000|         HR|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|\n",
      "| 10|   Jack| 33| 80000|Engineering|\n",
      "| 11|   Kate| 26| 50000|         HR|\n",
      "| 12|   Lily| 34| 75000|  Marketing|\n",
      "| 13|   Mike| 28| 60000|Engineering|\n",
      "| 14|  Nancy| 30| 70000|         HR|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|\n",
      "+---+-------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data/sample.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34706f6e-a2db-4319-a549-046e347b95cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbc545-cf72-48ca-be2e-09f282adfcb2",
   "metadata": {},
   "source": [
    "## Window Functions - More advanced \"Aggregations\"\n",
    "\n",
    "> Window functions allow you to perform calculations across a set of rows that are related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e191b9f-3054-45e7-a9d3-83a71133f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+-----------+\n",
      "| id|   name|age|salary| department|salary_rank|\n",
      "+---+-------+---+------+-----------+-----------+\n",
      "|  4|  David| 40| 90000|Engineering|          1|\n",
      "|  2|    Bob| 35| 80000|Engineering|          2|\n",
      "| 10|   Jack| 33| 80000|Engineering|          2|\n",
      "| 13|   Mike| 28| 60000|Engineering|          4|\n",
      "|  7|   Gina| 27| 55000|Engineering|          5|\n",
      "|  1|  Alice| 30| 70000|         HR|          1|\n",
      "|  8|  Harry| 31| 70000|         HR|          1|\n",
      "| 14|  Nancy| 30| 70000|         HR|          1|\n",
      "|  5|    Eva| 28| 60000|         HR|          4|\n",
      "| 11|   Kate| 26| 50000|         HR|          5|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|          1|\n",
      "|  6|  Frank| 32| 75000|  Marketing|          2|\n",
      "| 12|   Lily| 34| 75000|  Marketing|          2|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|          4|\n",
      "|  3|Charlie| 25| 50000|  Marketing|          5|\n",
      "+---+-------+---+------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 19:02:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Define a window partitioned by 'department' and ordered by 'salary' in descending order\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply the window function to compute the rank of employees within their department\n",
    "df_with_rank = df.withColumn(\"salary_rank\", rank().over(window_spec))\n",
    "df_with_rank.show()\n",
    "\n",
    "# The 'Window.partitionBy' divides data into groups (departments),\n",
    "# and 'orderBy' sorts employees within each group by salary in desc order. \n",
    "# The 'rank' function assigns a rank to each row based on the order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5f69b-f50b-4a0a-b405-c68c99ae6c27",
   "metadata": {},
   "source": [
    "## Custom transformations using User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5276f6bd-d89f-4dee-b0fd-7433eafd94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+---------------+\n",
      "| id|   name|age|salary| department|salary_category|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "|  1|  Alice| 30| 70000|         HR|         Medium|\n",
      "|  2|    Bob| 35| 80000|Engineering|           High|\n",
      "|  3|Charlie| 25| 50000|  Marketing|            Low|\n",
      "|  4|  David| 40| 90000|Engineering|           High|\n",
      "|  5|    Eva| 28| 60000|         HR|         Medium|\n",
      "|  6|  Frank| 32| 75000|  Marketing|         Medium|\n",
      "|  7|   Gina| 27| 55000|Engineering|            Low|\n",
      "|  8|  Harry| 31| 70000|         HR|         Medium|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|         Medium|\n",
      "| 10|   Jack| 33| 80000|Engineering|           High|\n",
      "| 11|   Kate| 26| 50000|         HR|            Low|\n",
      "| 12|   Lily| 34| 75000|  Marketing|         Medium|\n",
      "| 13|   Mike| 28| 60000|Engineering|         Medium|\n",
      "| 14|  Nancy| 30| 70000|         HR|         Medium|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|           High|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# custom function that classifies salary\n",
    "def classify_salary(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"High\"\n",
    "    elif salary >= 60000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "classify_salary_udf = udf(classify_salary, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column 'salary_category'\n",
    "df_with_category = df.withColumn(\"salary_category\", classify_salary_udf(col(\"salary\")))\n",
    "df_with_category.show()\n",
    "\n",
    "# The UDF 'classify_salary' categorizes each employee's salary.\n",
    "# UDFs should be used when built-in functions do not meet our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3ab53-d09e-41de-8eb2-2bc26c1ea457",
   "metadata": {},
   "source": [
    "### Efficient Join operations - using Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c625d6-a0ee-4b21-82da-e3a77b17c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------+---+------+-------------+\n",
      "| department| id|   name|age|salary|     location|\n",
      "+-----------+---+-------+---+------+-------------+\n",
      "|         HR|  1|  Alice| 30| 70000|     New York|\n",
      "|Engineering|  2|    Bob| 35| 80000|San Francisco|\n",
      "|  Marketing|  3|Charlie| 25| 50000|      Chicago|\n",
      "|Engineering|  4|  David| 40| 90000|San Francisco|\n",
      "|         HR|  5|    Eva| 28| 60000|     New York|\n",
      "|  Marketing|  6|  Frank| 32| 75000|      Chicago|\n",
      "|Engineering|  7|   Gina| 27| 55000|San Francisco|\n",
      "|         HR|  8|  Harry| 31| 70000|     New York|\n",
      "|  Marketing|  9|    Ivy| 29| 60000|      Chicago|\n",
      "|Engineering| 10|   Jack| 33| 80000|San Francisco|\n",
      "|         HR| 11|   Kate| 26| 50000|     New York|\n",
      "|  Marketing| 12|   Lily| 34| 75000|      Chicago|\n",
      "|Engineering| 13|   Mike| 28| 60000|San Francisco|\n",
      "|         HR| 14|  Nancy| 30| 70000|     New York|\n",
      "|  Marketing| 15|  Oscar| 32| 80000|      Chicago|\n",
      "+-----------+---+-------+---+------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a small DataFrame for department details\n",
    "dept_data = [\n",
    "    Row(department=\"HR\", location=\"New York\"),\n",
    "    Row(department=\"Engineering\", location=\"San Francisco\"),\n",
    "    Row(department=\"Marketing\", location=\"Chicago\")\n",
    "]\n",
    "dept_df = spark.createDataFrame(dept_data)\n",
    "\n",
    "# Perform a broadcast join between the employee DataFrame and the department DataFrame\n",
    "joined_df = df.join(broadcast(dept_df), on=\"department\", how=\"left\")\n",
    "joined_df.show()\n",
    "\n",
    "# The 'broadcast' function hints to Spark that dept_df is small, \n",
    "\n",
    "# and it should be broadcasted to all worker nodes for a more efficient join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f502ed4-b2df-4d3c-a0bd-6d727ffc1eac",
   "metadata": {},
   "source": [
    "### Caching and Persisting intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aaf5b41-ea50-4d9f-9349-4a2b424c84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-----------+---------------+\n",
      "| id|   name|age|salary| department|salary_category|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "|  1|  Alice| 30| 70000|         HR|         Medium|\n",
      "|  2|    Bob| 35| 80000|Engineering|           High|\n",
      "|  3|Charlie| 25| 50000|  Marketing|            Low|\n",
      "|  4|  David| 40| 90000|Engineering|           High|\n",
      "|  5|    Eva| 28| 60000|         HR|         Medium|\n",
      "|  6|  Frank| 32| 75000|  Marketing|         Medium|\n",
      "|  7|   Gina| 27| 55000|Engineering|            Low|\n",
      "|  8|  Harry| 31| 70000|         HR|         Medium|\n",
      "|  9|    Ivy| 29| 60000|  Marketing|         Medium|\n",
      "| 10|   Jack| 33| 80000|Engineering|           High|\n",
      "| 11|   Kate| 26| 50000|         HR|            Low|\n",
      "| 12|   Lily| 34| 75000|  Marketing|         Medium|\n",
      "| 13|   Mike| 28| 60000|Engineering|         Medium|\n",
      "| 14|  Nancy| 30| 70000|         HR|         Medium|\n",
      "| 15|  Oscar| 32| 80000|  Marketing|           High|\n",
      "+---+-------+---+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame after performing a transformation\n",
    "cached_df = df_with_category.cache()\n",
    "cached_df.show()\n",
    "\n",
    "# Using 'cache()' will persist the DataFrame in memory, reducing the time needed for subsequent actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83654dcc-c204-411e-af74-435cf29619c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
